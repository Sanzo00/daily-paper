import argparse
import datetime
import json
import logging
import os
import re
import time

import arxiv
import requests
import yaml

logging.basicConfig(
    format="[%(asctime)s %(levelname)s] %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    level=logging.INFO,
)

base_url = "https://arxiv.paperswithcode.com/api/v0/papers/"
github_url = "https://api.github.com/search/repositories"
arxiv_url = "http://arxiv.org/"


def load_config(config_file: str) -> dict:
    """
    config_file: input config file path
    return: a dict of configuration
    """

    # make filters pretty
    def pretty_filters(**config) -> dict:
        keywords = dict()

        def build_query(filters, logic):
            # 单组字符串列表
            if isinstance(filters[0], str):
                if logic == "AND":
                    return [
                        " AND ".join([f'"{f}"' if " " in f else f for f in filters])
                    ]
                else:
                    return [" OR ".join([f'"{f}"' if " " in f else f for f in filters])]
            # 多组组合，例如 filters = [[A, B], [C, D]]
            elif isinstance(filters[0], list):
                return [
                    " AND ".join([f'"{f}"' if " " in f else f for f in group])
                    for group in filters
                ]
            return []

        for topic, conf in config["keywords"].items():
            if isinstance(conf, dict):
                logic = conf.get("logic", "OR")
                filters = conf.get("filters", [])
            else:
                logic = "OR"
                filters = conf

            queries = build_query(filters, logic)
            keywords[topic] = queries  # Note: value is now list of query strings

        return keywords

    with open(config_file, "r") as f:
        config = yaml.load(f, Loader=yaml.FullLoader)
        config["kv"] = pretty_filters(**config)
        logging.info(f"config = {config}")
    return config


def get_authors(authors, first_author=False):
    output = str()
    if first_author == False:
        output = ", ".join(str(author) for author in authors)
    else:
        output = authors[0]
    return output


def sort_papers(papers):
    output = dict()
    keys = list(papers.keys())
    keys.sort(reverse=True)
    for key in keys:
        output[key] = papers[key]
    return output


def get_code_link(qword: str) -> str:
    """
    This short function was auto-generated by ChatGPT.
    I only renamed some params and added some comments.
    @param qword: query string, eg. arxiv ids and paper titles
    @return paper_code in github: string, if not found, return None
    """
    # query = f"arxiv:{arxiv_id}"
    query = f"{qword}"
    params = {"q": query, "sort": "stars", "order": "desc"}
    r = requests.get(github_url, params=params)
    results = r.json()
    code_link = None
    if results["total_count"] > 0:
        code_link = results["items"][0]["html_url"]
    return code_link


# fmt: off
# 常见会议简称（按领域分组；尽量去重）
CONFS_BY_FIELD = {
    # 数据库 / 数据挖掘 / 信息检索
    "DB/DM/IR": [
        "SIGMOD","VLDB","ICDE","PODS","CIDR",
        "KDD","ICDM","SDM","PAKDD","ECML-PKDD","RecSys",
        "SIGIR","CIKM","WSDM","ECIR","EDBT","DASFAA","ISWC","ICDT","WAIM","SSDBM","SSTD","WebDB","WISE","MDM","ADMA","APWeb"
    ],
    # 人工智能 / 机器学习 / NLP / CV / 机器人
    "AI/ML/NLP/CV/Robotics": [
        "AAAI","IJCAI","NeurIPS","NIPS","ICML","ICLR","AISTATS","UAI","COLT","ALT",
        "ACL","EMNLP","NAACL","EACL","COLING","TACL","CoNLL","NLPCC","ACML",
        "CVPR","ICCV","ECCV","BMVC","ACCV",
        "ICRA","IROS","FG","ICPR","IJCB"
    ],
    # 体系结构 / 并行与分布式 / 操作系统 / 编程语言 / 软件工程 / 系统
    "Systems/PL/SE/Arch": [
        "ASPLOS","ISCA","MICRO","HPCA","SC","PPoPP","FAST","DAC","ATC","EuroSys","USENIX ATC","VEE","SOCC",
        "OSDI","SOSP","HotOS","HotStorage","HotEdge","Middleware","LISA",
        "PLDI","POPL","OOPSLA","ICFP","SAS","VMCAI","LCTES","CGO","PACT","HiPEAC","SPAA","PODC","ICS","ICPP","IPDPS","HPDC","CLUSTER","Euro-Par","HiPC","ICDCS","MSST","RTAS","Performance","VEE","DATE","FPGA","CODES+ISSS","CASES","ASP-DAC","FPT","ISPASS"
    ],
    # 计算机网络
    "Networking": [
        "SIGCOMM","NSDI","INFOCOM","MobiCom","CoNEXT","SenSys","IPSN","IMC",
        "MobiSys","ICNP","MobiHoc","IWQoS","SECON","NOSSDAV","APNet","HotNets","WCNC","GLOBECOM","ICC","LCN","Networking","WoWMoM","ANCS","APNOMS"
    ],
    # 网络与信息安全 / 密码学 / 可靠性
    "Security/Crypto": [
        "S&P","IEEE S&P","Oakland","USENIX Security","CCS","NDSS",
        "CRYPTO","EUROCRYPT","ASIACRYPT","TCC","PKC","FSE","CHES",
        "RAID","DSN","SRDS","PETS","CSFW","EuroS&P"
    ],
    # 人机交互 / 普适计算 / 社会计算
    "HCI/UbiComp": [
        "CHI","UIST","CSCW","UbiComp","IMWUT","IUI","MobileHCI","ISS","ECSCW","PERCOM","ICWSM","DIS","ICMI","ASSETS","AVI","GROUP"
    ],
    # 图形学 / 多媒体 / 可视化 / 信号图像
    "Graphics/Multimedia/Vis": [
        "SIGGRAPH","Eurographics","EGSR","SGP","SPM","EuroVis","IEEE VIS",
        "ACM MM","ICMR","ICME","ICASSP","ICIP","3DV","PacificVis","DCC","MMAsia","PG","SMI"
    ],
    # 计算机科学理论
    "Theory": [
        "STOC","FOCS","SODA","LICS","CAV","ICALP","ESA","CCC","SAT","HSCC","CONCUR","SoCG","COCOON","ISAAC","MFCS","STACS","IPCO"
    ],
    # 交叉 / Web / 空间信息
    "Cross/Web/Geo": [
        "WWW","TheWebConf","WWW Companion","SIGSPATIAL","WINE","MICCAI","RECOMB","BIBM","CogSci","IR-RAG"
    ],
}
# fmt: on

# 展平为去重列表（如果你还需要一个平面集合用于简单 in 判断）
CONFS = sorted({c for lst in CONFS_BY_FIELD.values() for c in lst})

URL_RE = re.compile(r"https?://[^\s)]+", re.IGNORECASE)


def parse_comment(comment: str | None):
    if not comment:
        return None, None

    text = " ".join(comment.split())  # 简单去掉多余空白

    urls = URL_RE.findall(text)

    url = urls[0] if len(urls) > 0 else None

    for u in urls:
        if "github" in u:
            url = u
            break

    venue = None
    lower_text = text.lower()
    for v in CONFS:
        if v.lower() in lower_text:
            venue = v
            break

    return url, venue


def get_daily_papers(topic, query="slam", max_results=5):
    """
    @param topic: str
    @param query: str
    @return paper_with_code: dict
    """
    # output
    content = dict()
    content_to_web = dict()
    search_engine = arxiv.Search(
        query=query, max_results=max_results, sort_by=arxiv.SortCriterion.SubmittedDate
    )

    error_count = 0
    max_errors = 5

    try:
        results = list(search_engine.results())
    except Exception as e:
        logging.error(f"Error while fetching results: {e}")
        return {topic: content}, {topic: content_to_web}

    for result in results:

        paper_id = result.get_short_id()
        paper_title = result.title
        paper_url = result.entry_id
        code_url = base_url + paper_id  # TODO
        paper_abstract = result.summary.replace("\n", " ")
        paper_authors = get_authors(result.authors)
        paper_first_author = get_authors(result.authors, first_author=True)
        primary_category = result.primary_category
        publish_time = result.published.date()
        update_time = result.updated.date()
        comments = result.comment

        logging.info(
            f"Time = {update_time} title = {paper_title} author = {paper_first_author}"
        )

        # eg: 2108.09112v1 -> 2108.09112
        ver_pos = paper_id.find("v")
        if ver_pos == -1:
            paper_key = paper_id
        else:
            paper_key = paper_id[0:ver_pos]
        paper_url = arxiv_url + "abs/" + paper_key

        repo_url, venue = parse_comment(comments)
        venue = "" if venue is None else "_(" + venue + ")"
        # if comments != None:
        #     content_to_web[paper_key] += f", {comments}\n"
        # else:
        #     content_to_web[paper_key] += f"\n"

        # 先假设 repo_url = None
        # repo_url = None

        if repo_url is None:
            if error_count < max_errors:
                try:
                    # source code link
                    r = requests.get(code_url, timeout=10).json()
                    error_count = 0  # 成功则清零

                    if "official" in r and r["official"]:
                        repo_url = r["official"]["url"]

                except Exception as e:
                    error_count += 1
                    logging.error(
                        f"exception: {e} with id: {paper_key} (error_count={error_count})"
                    )

            else:
                logging.warning(
                    f"Skip API request due to {error_count} consecutive errors"
                )

        # 统一写入 content / content_to_web
        if repo_url is not None:
            content[paper_key] = (
                "|**{}**|**{}**|{} et.al.|[{}{}]({})|**[link]({})**|\n".format(
                    update_time,
                    paper_title,
                    paper_first_author,
                    paper_key,
                    venue,
                    paper_url,
                    repo_url,
                )
            )
            content_to_web[paper_key] = (
                "- {}, **{}**, {} et.al., Paper: [{}{}]({}), Code: **[{}]({})**".format(
                    update_time,
                    paper_title,
                    paper_first_author,
                    paper_url,
                    venue,
                    paper_url,
                    repo_url,
                    repo_url,
                )
            )
        else:
            content[paper_key] = "|**{}**|**{}**|{} et.al.|[{}{}]({})|null|\n".format(
                update_time,
                paper_title,
                paper_first_author,
                paper_key,
                venue,
                paper_url,
            )
            content_to_web[paper_key] = (
                "- {}, **{}**, {} et.al., Paper: [{}{}]({})".format(
                    update_time,
                    paper_title,
                    paper_first_author,
                    paper_url,
                    venue,
                    paper_url,
                )
            )

    data = {topic: content}
    data_web = {topic: content_to_web}
    return data, data_web


def update_paper_links(filename):
    """
    weekly update paper links in json file
    """

    def parse_arxiv_string(s):
        parts = s.split("|")
        date = parts[1].strip()
        title = parts[2].strip()
        authors = parts[3].strip()
        arxiv_id = parts[4].strip()
        code = parts[5].strip()
        arxiv_id = re.sub(r"v\d+", "", arxiv_id)
        return date, title, authors, arxiv_id, code

    with open(filename, "r") as f:
        content = f.read()
        if not content:
            m = {}
        else:
            m = json.loads(content)

        json_data = m.copy()

        for keywords, v in json_data.items():
            logging.info(f"keywords = {keywords}")
            for paper_id, contents in v.items():
                contents = str(contents)

                update_time, paper_title, paper_first_author, paper_url, code_url = (
                    parse_arxiv_string(contents)
                )

                contents = "|{}|{}|{}|{}|{}|\n".format(
                    update_time, paper_title, paper_first_author, paper_url, code_url
                )
                json_data[keywords][paper_id] = str(contents)
                logging.info(f"paper_id = {paper_id}, contents = {contents}")

                valid_link = False if "|null|" in contents else True
                if valid_link:
                    continue
                try:
                    code_url = base_url + paper_id  # TODO
                    r = requests.get(code_url).json()
                    repo_url = None
                    if "official" in r and r["official"]:
                        repo_url = r["official"]["url"]
                        if repo_url is not None:
                            new_cont = contents.replace(
                                "|null|", f"|**[link]({repo_url})**|"
                            )
                            logging.info(f"ID = {paper_id}, contents = {new_cont}")
                            json_data[keywords][paper_id] = str(new_cont)

                except Exception as e:
                    logging.error(f"exception: {e} with id: {paper_id}")
        # dump to json file
        with open(filename, "w") as f:
            json.dump(json_data, f)


def update_json_file(filename, data_dict):
    """
    daily update json file using data_dict
    """
    with open(filename, "r") as f:
        content = f.read()
        if not content:
            m = {}
        else:
            m = json.loads(content)

    json_data = m.copy()

    # update papers in each keywords
    for data in data_dict:
        for keyword in data.keys():
            papers = data[keyword]

            if keyword in json_data.keys():
                json_data[keyword].update(papers)
            else:
                json_data[keyword] = papers

    with open(filename, "w") as f:
        json.dump(json_data, f)


def json_to_md(
    filename,
    md_filename,
    task="",
    to_web=False,
    use_title=True,
    use_tc=True,
    use_b2t=True,
):
    """
    @param filename: str
    @param md_filename: str
    @return None
    """

    def pretty_math(s: str) -> str:
        ret = ""
        match = re.search(r"\$.*\$", s)
        if match == None:
            return s
        math_start, math_end = match.span()
        space_trail = space_leading = ""
        if s[:math_start][-1] != " " and "*" != s[:math_start][-1]:
            space_trail = " "
        if s[math_end:][0] != " " and "*" != s[math_end:][0]:
            space_leading = " "
        ret += s[:math_start]
        ret += f"{space_trail}${match.group()[1:-1].strip()}${space_leading}"
        ret += s[math_end:]
        return ret

    DateNow = datetime.date.today()
    DateNow = str(DateNow)
    DateNow = DateNow.replace("-", ".")

    with open(filename, "r") as f:
        content = f.read()
        if not content:
            data = {}
        else:
            data = json.loads(content)

    # clean README.md if daily already exist else create it
    with open(md_filename, "w+") as f:
        pass

    # write data into README.md
    with open(md_filename, "a+") as f:

        if (use_title == True) and (to_web == True):
            f.write("---\n" + "layout: default\n" + "---\n\n")

        if use_title == True:
            f.write("## Updated on " + DateNow + "\n")
        else:
            f.write("> Updated on " + DateNow + "\n")

        # Add: table of contents
        if use_tc == True:
            f.write("<details>\n")
            f.write("  <summary>Table of Contents</summary>\n")
            f.write("  <ol>\n")
            for keyword in data.keys():
                day_content = data[keyword]
                if not day_content:
                    continue
                kw = keyword.replace(" ", "-")
                f.write(f"    <li><a href=#{kw.lower()}>{keyword}</a></li>\n")
            f.write("  </ol>\n")
            f.write("</details>\n\n")

        for keyword in data.keys():
            day_content = data[keyword]
            if not day_content:
                continue
            # the head of each part
            f.write(f"## {keyword}\n\n")

            if use_title == True:
                if to_web == False:
                    f.write(
                        "|Publish Date|Title|Authors|PDF|Code|\n"
                        + "|---|---|---|---|---|\n"
                    )
                else:
                    f.write("| Publish Date | Title | Authors | PDF | Code |\n")
                    f.write(
                        "|:---------|:-----------------------|:---------|:------|:------|\n"
                    )

            # sort papers by date
            day_content = sort_papers(day_content)

            for _, v in day_content.items():
                if v is not None:
                    f.write(pretty_math(v))  # make latex pretty

            f.write(f"\n")

            # Add: back to top
            if use_b2t:
                top_info = f"#Updated on {DateNow}"
                top_info = top_info.replace(" ", "-").replace(".", "")
                f.write(
                    f"<p align=right>(<a href={top_info.lower()}>back to top</a>)</p>\n\n"
                )

    logging.info(f"{task} finished")


# def json_to_md_split(filename, output_dir,
#                      to_web=False, use_title=True,
#                      use_tc=False, use_b2t=False):
#     """
#     将 JSON 数据中每个分类导出为独立 Markdown 文件，适用于 Just-the-Docs 多页面导航结构。

#     @param filename: JSON 文件路径（结构如 {"LLM Reasoning": {...}, ...}）
#     @param output_dir: 保存 markdown 的文件夹（如 docs/）
#     """

#     import os
#     DateNow = datetime.date.today().strftime("%Y.%m.%d")

#     def pretty_math(s: str) -> str:
#         # 美化 LaTeX 数学公式
#         ret = ''
#         match = re.search(r"\$.*\$", s)
#         if match is None:
#             return s
#         math_start, math_end = match.span()
#         space_trail = space_leading = ''
#         if s[:math_start][-1] != ' ' and '*' != s[:math_start][-1]: space_trail = ' '
#         if s[math_end:][0] != ' ' and '*' != s[math_end:][0]: space_leading = ' '
#         ret += s[:math_start]
#         ret += f'{space_trail}${match.group()[1:-1].strip()}${space_leading}'
#         ret += s[math_end:]
#         return ret

#     # 加载 JSON 文件
#     with open(filename, "r") as f:
#         content = f.read()
#         if not content:
#             data = {}
#         else:
#             data = json.loads(content)

#     for keyword, papers in data.items():
#         if not papers:
#             continue

#         # 生成 markdown 文件名，如 LLM Reasoning -> llm_reasoning.md
#         safe_name = keyword.lower().replace(" ", "_")
#         md_path = os.path.join(output_dir, f"{safe_name}.md")

#         with open(md_path, "w") as f:
#             if use_title and to_web:
#                 f.write("---\nlayout: default\nnav_order: 1\n---\n\n")
#             if use_title:
#                 f.write(f"# {keyword}\n\n")
#                 f.write(f"_Updated on {DateNow}_\n\n")
#             else:
#                 f.write(f"> Updated on {DateNow}\n\n")

#             if use_title:
#                 if to_web:
#                     f.write("| Publish Date | Title | Authors | PDF | Code |\n")
#                     f.write("|:-------------|:------|:--------|:----|:-----|\n")
#                 else:
#                     f.write("|Publish Date|Title|Authors|PDF|Code|\n")
#                     f.write("|---|---|---|---|---|\n")

#             papers = sort_papers(papers)
#             for _, v in papers.items():
#                 if v is not None:
#                     f.write(pretty_math(v))

#             if use_b2t:
#                 f.write(f"<p align=right>(<a href=#top>back to top</a>)</p>\n\n")

#         logging.info(f"[{keyword}] saved to {md_path}")


def json_to_multi_md(json_file, output_dir):
    """
    将 arxiv_web.json 拆成多个 Markdown 文件，每个主题一个，适配 Just the Docs。
    """
    os.makedirs(output_dir, exist_ok=True)

    with open(json_file, "r") as f:
        data = json.load(f)

    today = str(datetime.date.today()).replace("-", ".")

    for i, (topic, papers) in enumerate(data.items(), start=1):
        if not papers:
            continue

        filename = topic.replace(" ", "_").lower() + ".md"
        if i == 1:
            filename = "index.md"
        path = os.path.join(output_dir, filename)

        with open(path, "w") as f:
            f.write(f"---\nlayout: default\ntitle: {topic}\nnav_order: {i}\n---\n\n")
            f.write(f"# {topic} Papers\n\n")
            f.write(f"_Updated on {today}_\n\n")
            f.write("| Publish Date | Title | Authors | PDF | Code |\n")
            f.write("|:-------------|:------|:--------|:----|:-----|\n")

            sorted_items = sorted(papers.items(), key=lambda x: x[1][:13], reverse=True)
            for _, content in sorted_items:
                f.write(content)


def demo(**config):
    # TODO: use config
    data_collector = []
    data_collector_web = []

    keywords = config["kv"]
    max_results = config["max_results"]
    publish_readme = config["publish_readme"]
    publish_gitpage = config["publish_gitpage"]

    b_update = config["update_paper_links"]
    logging.info(f"Update Paper Link = {b_update}")
    if config["update_paper_links"] == False:
        logging.info(f"GET daily papers begin")
        for topic, keywords in keywords.items():
            logging.info(f"Keyword: {topic}")

            for query in keywords:
                data, data_web = get_daily_papers(
                    topic, query=query, max_results=max_results
                )

                data_collector.append(data)
                data_collector_web.append(data_web)
                print("\n")

                # 1. update README.md file
                if publish_readme:
                    json_file = config["json_readme_path"]
                    md_file = config["md_readme_path"]
                    # update paper links
                    if config["update_paper_links"]:
                        update_paper_links(json_file)
                    else:
                        # update json data
                        update_json_file(json_file, data_collector)
                    # json data to markdown
                    json_to_md(json_file, md_file, task="Update Readme")

                # 2. update docs/index.md file (to gitpage)
                # if publish_gitpage:
                #     json_file = config['json_gitpage_path']
                #     md_file   = config['md_gitpage_path']
                #     # TODO: duplicated update paper links!!!
                #     if config['update_paper_links']:
                #         update_paper_links(json_file)
                #     else:
                #         update_json_file(json_file,data_collector)
                #     json_to_md(json_file, md_file, task ='Update GitPage', \
                #         to_web = True, use_tc=False, use_b2t=False)

                if publish_gitpage:
                    json_file = config["json_gitpage_path"]
                    md_file = config["md_gitpage_path"]
                    if config["update_paper_links"]:
                        update_paper_links(json_file)
                    else:
                        update_json_file(json_file, data_collector)

                    # ✅ 改为多页面输出：
                    json_to_multi_md(json_file, output_dir="docs/")

                    # # ✅ index.md 提供总目录
                    # with open("docs/index.md", "w") as f:
                    #     f.write("---\nlayout: default\ntitle: Home\nnav_order: 1\n---\n\n")
                    #     f.write("# LLM Arxiv Daily\n\n")
                    #     f.write("## Topics\n\n")
                    #     for topic in data_collector_web[0].keys():
                    #         filename = topic.replace(" ", "_").lower() + ".md"
                    #         f.write(f"- [{topic}](topics/{filename})\n")

                logging.info(f"sleep 3s ...")
                time.sleep(3)
        logging.info(f"GET daily papers end")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--config_path", type=str, default="config.yaml", help="configuration file path"
    )
    parser.add_argument(
        "--update_paper_links",
        default=False,
        action="store_true",
        help="whether to update paper links etc.",
    )
    args = parser.parse_args()
    config = load_config(args.config_path)
    config = {**config, "update_paper_links": args.update_paper_links}
    demo(**config)
